{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6e7e56",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-10T09:25:30.677239Z"
    },
    "jupyter": {
     "is_executing": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a15fe31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! /get/notebook/lsy/env/bin/python -m pip install feather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b93354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:17:43.641639Z",
     "start_time": "2025-01-10T17:17:43.637690Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/get/smartmem_data/pyenv/lib/python3.11/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/home/ais/.cache/matplotlib is not a writable directory\n",
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-xiekw1h9 because there was an issue with the default path (/home/ais/.cache/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from pyarrow import feather\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deb6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fee52c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:17:44.730425Z",
     "start_time": "2025-01-10T17:17:44.724431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load ticket data\n",
    "ticket = pd.read_csv('./ticket.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0626a74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:17:48.738835Z",
     "start_time": "2025-01-10T17:17:48.733617Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sn_name</th>\n",
       "      <th>alarm_time</th>\n",
       "      <th>sn_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sn_4191</td>\n",
       "      <td>1704077850</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sn_10692</td>\n",
       "      <td>1704140121</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sn_7219</td>\n",
       "      <td>1704148156</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sn_31281</td>\n",
       "      <td>1704151660</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sn_8854</td>\n",
       "      <td>1704159849</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sn_name  alarm_time sn_type\n",
       "0   sn_4191  1704077850       A\n",
       "1  sn_10692  1704140121       A\n",
       "2   sn_7219  1704148156       A\n",
       "3  sn_31281  1704151660       A\n",
       "4   sn_8854  1704159849       A"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the ticket data\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d4d8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e6fadf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:22:17.482212Z",
     "start_time": "2025-01-10T17:22:17.451998Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load and process training data for Server Type A\n",
    "def load_data():\n",
    "    # trainset_A_path = './SmartHW-main/sample_data/server_type_A'\n",
    "    trainset_A = []\n",
    "    for file in glob.glob('/data1/smartmem/stage1_feather/*/*.feather'):\n",
    "        if file.endswith('.feather'):\n",
    "            df = pd.read_feather(file)  # Use pandas.read_feather()\n",
    "            df['sn'] = os.path.basename(file).split('.')[0]  # Extract filename without extension\n",
    "            trainset_A.append(df)\n",
    "            \n",
    "\n",
    "    # for filename in os.listdir(trainset_A_path):\n",
    "    #     file_path = os.path.join(trainset_A_path, filename)\n",
    "    #     if filename.endswith('.feather'):  # Check for Feather files\n",
    "    #         df = pd.read_feather(file_path)  # Use pandas.read_feather()\n",
    "    #         df['sn'] = os.path.splitext(filename)[0]  # Extract filename without extension\n",
    "    #         trainset_A.append(df)\n",
    "\n",
    "    trainset_A_df = pd.concat(trainset_A, ignore_index=True)\n",
    "    return trainset_A_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc25015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset_A_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03f6e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset_A_df= trainset_A_df[trainset_A_df.LogTime<pd.to_datetime('2024-06-01').timestamp()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eefded6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:22:18.736122Z",
     "start_time": "2025-01-10T17:22:18.732068Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Label the data based on anomalies\n",
    "# trainset_A_df['anomaly'] = 0\n",
    "# trainset_A_df.loc[trainset_A_df['sn'].isin(ticket['sn_name']), 'anomaly'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be658e01",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Error Counts Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00fd1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_time_windows = [1,\n",
    "                        5,\n",
    "                       30,\n",
    "                        60,\n",
    "                       5*60,\n",
    "                       10*60,\n",
    "                       15*60,\n",
    "                       30*60,\n",
    "                       1*3600,\n",
    "                       24*3600,\n",
    "                      24*2*3600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351ed8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df = df.sort_values(['sn','LogTime'])\n",
    "    df[\"time_index\"] = np.ceil(df[\"LogTime\"]/1)\n",
    "    df['time_diff'] = df.groupby(['sn']).LogTime.diff()\n",
    "    df['address'] = df['CpuId'].astype(str) + '-' + df['ChannelId'].astype(str) + '-' + df['RankId'].astype(str) + '-' + \\\n",
    "                    df['deviceID'].astype(str) + '-' + df['BankgroupId'].astype(str) + '-' + df['BankId'].astype(str) + '-' + \\\n",
    "                    df['ColumnId'].astype(str) + '-' + df['RowId'].astype(str)\n",
    "    df['bank_column_key'] = df['CpuId'].astype(str) + '-' + df['ChannelId'].astype(str) + '-' + df['RankId'].astype(str) + '-' + \\\n",
    "                            df['deviceID'].astype(str) + '-' + df['BankgroupId'].astype(str) + '-' + df['BankId'].astype(str) + '-' + df['ColumnId'].astype(str)\n",
    "    df['bank_row_key'] = df['CpuId'].astype(str) + '-' + df['ChannelId'].astype(str) + '-' + df['RankId'].astype(str) + '-' + \\\n",
    "                         df['deviceID'].astype(str) + '-' + df['BankgroupId'].astype(str) + '-' + df['BankId'].astype(str) + '-' + df['RowId'].astype(str)\n",
    "    df['bank_key'] = df['CpuId'].astype(str) + '-' + df['ChannelId'].astype(str) + '-' + df['RankId'].astype(str) + '-' + \\\n",
    "                     df['deviceID'].astype(str) + '-' + df['BankgroupId'].astype(str) + '-' + df['BankId'].astype(str)\n",
    "    df['Bankgroup_key'] = df['CpuId'].astype(str) + '-' + df['ChannelId'].astype(str) + '-' + df['RankId'].astype(str) + '-' + \\\n",
    "                     df['deviceID'].astype(str) + '-' + df['BankgroupId'].astype(str)\n",
    "    df['device_key'] = df['CpuId'].astype(str) + '-' + df['ChannelId'].astype(str) + '-' + df['RankId'].astype(str) + '-' + \\\n",
    "                     df['deviceID'].astype(str) \n",
    "    df['rank_key'] = df['CpuId'].astype(str) + '-' + df['ChannelId'].astype(str) + '-' + df['RankId'].astype(str)\n",
    "    df['CE_READ'] = df.error_type_full_name.str.contains('CE.READ')\n",
    "    df['CE_SCRUB'] = df.error_type_full_name.str.contains('CE.SCRUB')\n",
    "    df['CE_OTHER'] = (df.CE_READ==False)&(df.CE_SCRUB==False)\n",
    "    \n",
    "    df.RetryRdErrLogParity.fillna(0,inplace=True)\n",
    "    df.RetryRdErrLog.fillna(0,inplace=True)\n",
    "    df[\"parity_valid\"] = df.RetryRdErrLog.astype(int)&0x0001 \n",
    "    df['RetryRdErrLogParity'] = df[['RetryRdErrLogParity','parity_valid']].apply(lambda x:x[0] if x[1]==1 else 0,axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da777fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset_A_df = preprocess_data(trainset_A_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "932cc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recent_data(df,row,time):\n",
    "    start_time = row['LogTime'] - pd.Timedelta(seconds=time)\n",
    "    end_time = row['LogTime']\n",
    "    time_mask_ = (df['LogTime'] >= start_time) & (df['LogTime'] <= end_time)\n",
    "    recent_data_ = df.loc[time_mask_]\n",
    "    df.at[index,'error_count_CE.READ_{}'.format(time)] = recent_data_['error_type_full_name'].value_counts().get('CE.READ', 0)\n",
    "    df.at[index, 'error_count_CE.SCRUB_{}'.format(time)] = recent_data_['error_type_full_name'].value_counts().get('CE.SCRUB', 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be39e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = feather.read_dataframe('./type_A//sn_1.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5994f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "addr_cols = ['bank_column_key','bank_row_key','bank_key','Bankgroup_key','device_key','rank_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe8704ba-6786-40c7-b8b1-fbecb6b94727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample['sn'] ='sn_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51dd6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample =preprocess_data(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adeee9b6-a12b-486a-9b8c-6f4c6ede9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.groupby(['sn','time_index2','bank_row_key').LogTime.max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27a9438f-e88b-4462-a0f9-0f6b05c733a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '&'.join(['2','1-2-1-9.0-0-2-129330'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07b5b4d-6537-4d6b-8f03-bdb7b3833be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.groupby(['sn','time_index2','bank_row_key'])['time_diff'].agg([np.mean,np.std,np.ptp]).reset_index().rename(\n",
    "#                                                                                             columns={'mean':'time_diff_mean{}'.format('&'.join(['2','1-2-1-9.0-0-2-129330'])),\n",
    "#                                                                                                      'std':'time_diff_std{}'.format('&'.join(['2','1-2-1-9.0-0-2-129330'])),\n",
    "#                                                                                                     'ptp':'time_diff_ptp{}'.format('&'.join(['2','1-2-1-9.0-0-2-129330'])),\n",
    "#                                                                                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f61d8120-5448-40dc-a98f-e72e186098eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"time_diff\"] = sample[\"LogTime\"].max()-sample[\"LogTime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c869be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"time_diff2\"] = np.ceil(sample[\"time_diff\"]/300)*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bed5eafa-8388-4af2-a840-2c2e270523d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"time_diff2\"] = pd.to_datetime(sample[\"time_diff2\"], unit='s')\n",
    "# sample[\"time_diff3\"] = pd.to_datetime(sample[\"time_diff\"], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ddb554-b755-4e9b-b154-e4c35f650cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e95fe517-b783-415d-a1b9-b0aad547d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"LogTime2\"] = pd.to_datetime(sample[\"LogTime\"], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "508f036b-9e4e-4e90-bcaa-c03eb2996881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"time_index\"] = np.ceil(sample['LogTime'] / 300) * 300\n",
    "# sample[\"time_index2\"] = pd.to_datetime(sample[\"time_index\"], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc662389-b154-4063-8fac-630e49a04c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"time_index\"] = np.ceil(sample['LogTime'] / 600) * 600\n",
    "# sample[\"time_index3\"] = pd.to_datetime(sample[\"time_index\"], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d2b94f0-902f-4627-b536-b0f25774b161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43440/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a055365d-b6f5-48e4-af2e-2ef475de57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[['LogTime','LogTime2','time_diff','time_diff3','time_diff2','time_index2','time_index3']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "313afb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_counts_features_v2(tmp2,gdf,time):\n",
    "    \n",
    "    res= pd.DataFrame() \n",
    "    tmp1= gdf[['CE_READ','CE_SCRUB','CE_OTHER']].sum().reset_index().rename(columns={'CE_READ':'error_count_READ_{}'.format(time),\n",
    "                                                                                                     'CE_SCRUB':'error_count_SCRUB_{}'.format(time),\n",
    "                                                                                                                'CE_OTHER':'error_count_OTHER_{}'.format(time),\n",
    "                                                                                                                    'error_type_full_name':'error_count_total_{}'.format(time)})\n",
    "    tmp1['error_count_total_{}'.format(time)] = gdf.error_type_full_name.count().reset_index()['error_type_full_name']\n",
    "    del tmp1['time_index']\n",
    "    tmp1[\"LogTime\"] = tmp2[\"LogTime\"]\n",
    "    return tmp1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5d729",
   "metadata": {},
   "source": [
    "### 以秒为最小时间单位，计算CE发生的时间间隔特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20bcdb40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_interval(x):\n",
    "    x = x.tolist()\n",
    "    return x[-1]-x[0]\n",
    "def count_(x):\n",
    "    return len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5fa951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.发生CE的时间间隔\n",
    "# 2.每个时间窗口内，平均间隔时间\n",
    "\n",
    "def calculate_time_features_v2(tmp2,gdf,time):\n",
    "    # res= pd.DataFrame() \n",
    "    # for time in select_time_windows[::-1]:\n",
    "    #     df[\"time_index\"] = np.ceil(df[\"LogTime\"]/time)\n",
    "    # df2 = df.dropna(subset=['time_diff'])\n",
    "    tmp1 = gdf['time_diff'].agg([np.mean,np.std,np.ptp,time_interval]).reset_index().rename(\n",
    "                                                                                            columns={'mean':'time_diff_mean{}'.format(time),\n",
    "                                                                                                     'std':'time_diff_std{}'.format(time),\n",
    "                                                                                                    'ptp':'time_diff_ptp{}'.format(time),\n",
    "                                                                                                    'time_interval':'time_interval{}'.format(time)})\n",
    "    # tmp2 = df2[['LogTime','time_index']].groupby('time_index').LogTime.max().reset_index()\n",
    "    del tmp1['time_index']\n",
    "    tmp1[\"LogTime\"] = tmp2[\"LogTime\"]\n",
    "        # if res.empty:\n",
    "        #     res = tmp1\n",
    "        # else:\n",
    "        #     res = res.merge(tmp1,on=['sn','LogTime'],how='right')\n",
    "            \n",
    "    return tmp1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f37de5",
   "metadata": {},
   "source": [
    "### 计算一个时间窗口内的Fault Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11956db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4364583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate fault features based on memory cell locations\n",
    "def calculate_faults_features_v2(tmp3,gdf,time,addr_):\n",
    "    res=pd.DataFrame()\n",
    "    tmp1 = gdf[['CE_READ' ,'CE_SCRUB','CE_OTHER']].sum().reset_index().rename(columns={'CE_READ':'{}_fault_ce_n_{}'.format(addr_,time),\n",
    "                                                                                        'CE_SCRUB':'{}_fault_sc_n_{}'.format(addr_,time),\n",
    "                                                                                        'CE_OTHER':'{}_fault_ot_n_{}'.format(addr_,time)})\n",
    "    tmp2 = gdf[['error_type_full_name']].nunique().reset_index().rename(columns={'error_type_full_name':'{}_total_fault_n_{}'.format(addr_,time)})\n",
    "    del tmp1[addr_]\n",
    "    # del tmp2[addr_]\n",
    "    tmp1= tmp1.merge(tmp2,on=['sn','time_index'],how='left')\n",
    "    del tmp1['time_index']\n",
    "    tmp1[\"LogTime\"] = tmp3[\"LogTime\"]\n",
    "\n",
    "     \n",
    "        # if res.empty:\n",
    "        #     res = tmp1\n",
    "        # else:\n",
    "        #     res = res.merge(tmp1,on=['sn','LogTime'],how='right')\n",
    "    return tmp1\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2073fef6",
   "metadata": {},
   "source": [
    "###LogTime计算CE计数的二级特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dd2039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_stastic_v2(tmp2,gdf,time,cols,addr):\n",
    "    res= pd.DataFrame() \n",
    "    # for time in select_time_windows[::-1]:\n",
    "    #     df[\"time_index\"] = np.ceil(df[\"LogTime\"]/time)\n",
    "    # if time == 1:\n",
    "    #     continue\n",
    "    tmp1 = gdf[cols].agg([np.mean,np.std,np.ptp]).reset_index()\n",
    "    tmp1.columns = ['_'.join(col) for col in tmp1.columns]\n",
    "\n",
    "    for col in tmp1.columns:\n",
    "        if col != \"sn_\" and col != \"time_index_\" and col != f\"{addr}_\":\n",
    "            tmp1.rename(columns={col:f\"{col}_{time}\"},inplace=True)\n",
    "        else:\n",
    "            tmp1.rename(columns={col:col[0:len(col)-1]},inplace=True)\n",
    "\n",
    "    # tmp2 = df[['LogTime','time_index']].groupby('time_index').LogTime.max().reset_index()\n",
    "    del tmp1['time_index']\n",
    "\n",
    "    tmp1[\"LogTime\"] = tmp2[\"LogTime\"]\n",
    "\n",
    "    # if res.empty:\n",
    "    #     res = tmp1\n",
    "    # else:\n",
    "    #     res = res.merge(tmp1,on=['sn','LogTime'],how='right')\n",
    "    return tmp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5c859f",
   "metadata": {},
   "source": [
    "### 错误计数变化量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e7d2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_features_diff1 = pd.concat([count_features[['sn','time_index']],count_features[count_features.columns.difference(['sn','time_index','LogTime'])].diff()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1292ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faults_features_diff1 = pd.concat([faults_features[['sn','time_index']],faults_features[faults_features.columns.difference(['sn','time_index','LogTime'])].diff()],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adecd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level2_features_v2(tmp2,count_features,faults_features,time,addr_):\n",
    "    cols1 = [f'error_count_READ_{time}', f'error_count_SCRUB_{time}', f'error_count_OTHER_{time}',f'error_count_total_{time}']\n",
    "    err_count_statistic_features = calculate_error_stastic_v2(tmp2,count_features,time,cols1,addr_)\n",
    "    cols2 = [f'{addr_}_fault_ce_n_{time}',f'{addr_}_fault_sc_n_{time}',f'{addr_}_fault_ot_n_{time}',f'{addr_}_total_fault_n_{time}',]\n",
    "    faults_statistic_features = calculate_error_stastic_v2(tmp2,faults_features,time,cols2,addr_)\n",
    "    return err_count_statistic_features,faults_statistic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac4360e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level2_diff_features_v2(count_features,faults_features):\n",
    "    tmp = count_features[count_features.columns.difference(['sn','time_index','LogTime'])].diff()\n",
    "    tmp.rename(columns=lambda x:f'{x}_d1',inplace=True)\n",
    "    count_features_diff1 = pd.concat([count_features[['sn','LogTime']],tmp],axis=1)\n",
    "    \n",
    "    tmp=faults_features[faults_features.columns.difference(['sn','time_index','LogTime'])].diff()\n",
    "    tmp.rename(columns=lambda x:f'{x}_d1',inplace=True)\n",
    "    faults_features_diff1 = pd.concat([faults_features[['sn','LogTime']],tmp],axis=1)\n",
    "    return count_features_diff1,faults_features_diff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4965cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,_,a,b=get_level2_features(counts_features,faults_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c2a3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc6781",
   "metadata": {},
   "source": [
    "### 计算时间窗口内发生CE风暴的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5348750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_ce_storm_count(\n",
    "    log_times: np.ndarray,\n",
    "    ce_storm_interval_seconds: int = 60,\n",
    "    ce_storm_count_threshold: int = 10,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    计算 CE 风暴的数量\n",
    "\n",
    "    CE 风暴定义:\n",
    "    - 首先定义相邻 CE 日志: 若两个 CE 日志 LogTime 时间间隔 < 60s, 则为相邻日志;\n",
    "    - 如果相邻日志的个数 >10, 则为发生 1 次 CE 风暴(注意: 如果相邻日志数量持续增长, 超过了 10, 则也只是记作 1 次 CE 风暴)\n",
    "\n",
    "    :param log_times: 日志 LogTime 列表\n",
    "    :param ce_storm_interval_seconds: CE 风暴的时间间隔阈值\n",
    "    :param ce_storm_count_threshold: CE 风暴的数量阈值\n",
    "    :return: CE风暴的数量\n",
    "    \"\"\"\n",
    "\n",
    "    log_times = sorted(log_times)\n",
    "    ce_storm_count = 0\n",
    "    consecutive_count = 0\n",
    "\n",
    "    for i in range(1, len(log_times)):\n",
    "        if log_times[i] - log_times[i - 1] <= ce_storm_interval_seconds:\n",
    "            consecutive_count += 1\n",
    "        else:\n",
    "            consecutive_count = 0\n",
    "        if consecutive_count > ce_storm_count_threshold:\n",
    "            ce_storm_count += 1\n",
    "            consecutive_count = 0\n",
    "\n",
    "    return ce_storm_count\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bcb117a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_storm_features_v2(tmp2,gdf,time):\n",
    "    res = pd.DataFrame()\n",
    "    # for time in select_time_windows[::-1]:\n",
    "    #     df[\"time_index\"] = np.ceil(df[\"LogTime\"]/time)\n",
    "    tmp1 = gdf.LogTime.apply(lambda x:_calculate_ce_storm_count(x)).reset_index().rename(columns={'LogTime':'ce_storm_count_{}'.format(time)})\n",
    "\n",
    "    # tmp2 = df[['LogTime','time_index']].groupby('time_index').LogTime.max().reset_index()\n",
    "    tmp1[\"LogTime\"] = tmp2[\"LogTime\"]\n",
    "    del tmp1['time_index']\n",
    "        # if res.empty:\n",
    "        #     res = tmp1\n",
    "        # else:\n",
    "        #     res = res.merge(tmp1,on=['sn','LogTime'],how='right')\n",
    "    return tmp1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6ab35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7325dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_positive_areas(grid):\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "    visited = [[False] * cols for _ in range(rows)]\n",
    "    areas = []  # 存储每个连通区域的面积\n",
    "\n",
    "    def dfs(r, c):\n",
    "        \"\"\" 深度优先搜索计算连通区域的面积 \"\"\"\n",
    "        if r < 0 or r >= rows or c < 0 or c >= cols:  # 越界\n",
    "            return 0\n",
    "        if visited[r][c] or grid[r][c] <= 0:  # 已访问或值不大于0\n",
    "            return 0\n",
    "        \n",
    "        visited[r][c] = True  # 标记访问\n",
    "        area = 1  # 当前点计入面积\n",
    "        \n",
    "        # 递归搜索四个方向\n",
    "        area += dfs(r + 1, c)  # 下\n",
    "        area += dfs(r - 1, c)  # 上\n",
    "        area += dfs(r, c + 1)  # 右\n",
    "        area += dfs(r, c - 1)  # 左\n",
    "        \n",
    "        return area\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if grid[i][j] > 0 and not visited[i][j]:  # 发现新的连通区域\n",
    "                area = dfs(i, j)\n",
    "                areas.append(area)  # 记录该区域的面积\n",
    "    return areas  # 返回总面积\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "075dcac9-636e-4eee-94d6-e4a1ab8f874d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1da69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrix_feature(data):\n",
    "    burst = []\n",
    "    dq = []\n",
    "    \n",
    "    for i in range(8):\n",
    "        if sum(data[i]) >0:\n",
    "            burst.append(i) \n",
    "            \n",
    "    for j in range(4):\n",
    "        if sum(data[:,j]) >0:\n",
    "            dq.append(j)  \n",
    "            \n",
    "    max_burst = burst[-1] - burst[0] if len(burst) else 0\n",
    "    max_dq = dq[-1] - dq[0] if len(dq) else 0\n",
    "    \n",
    "    err_dq_count = len(dq)\n",
    "    err_burst_count = len(burst)\n",
    "\n",
    "    err_row=[]\n",
    "    for i in range(8):\n",
    "        if sum(data[i]>0) >1:\n",
    "            err_row.append(sum(data[i]>0))\n",
    "    err_raw_count = len(err_row)\n",
    "    \n",
    "  \n",
    "    err_raw_count_max = sorted(err_row)[-1] if len(err_row)>0 else 0\n",
    "\n",
    "    err_col=[]\n",
    "    for j in range(4):\n",
    "        if sum(data[:,j]>0) >1:\n",
    "            err_col.append(sum(data[:,j]>0)) \n",
    "    err_col_count = len(err_col)\n",
    "\n",
    "    err_col_count_max = sorted(err_col)[-1] if len(err_col)>0 else 0\n",
    "\n",
    "    areas = compute_positive_areas(data.tolist())\n",
    "\n",
    "    max_adj_areas = max(areas) if len(areas) else 0 \n",
    "    total_areas = sum(areas) if len(areas) else 0\n",
    "    areas_count = len(areas)\n",
    "    return [max_adj_areas,total_areas,areas_count,max_burst,max_dq,err_raw_count,err_col_count,err_dq_count,err_burst_count,err_raw_count_max,err_col_count_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3a5bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrix(paritys):\n",
    "    res = np.zeros([8,4])\n",
    "    def _process(parity):\n",
    "        bin_parity = bin(int(parity))[2:].zfill(32)\n",
    "        return np.array([[int(j) for j in bin_parity[i : i + 4]] for i in range(0, 32, 4)])\n",
    "    for parity in paritys:\n",
    "        res += _process(parity)\n",
    "    return get_metrix_feature(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe4091b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bit_features2_v2(tmp2,gdf,time):\n",
    "    res = pd.DataFrame()\n",
    "    # print(gdf.RetryRdErrLogParity.apply(lambda x:pd.DataFrame([process_metrix(x)])).reset_index())\n",
    "    tmp1= gdf.RetryRdErrLogParity.apply(lambda x:pd.DataFrame([process_metrix(x)])).reset_index().drop(columns=['level_3']).rename(\n",
    "                                                    columns=dict(zip([0,1,2,3,4,5,6,7,8,9,10],\n",
    "                                                           ['max_adj_areas_{}'.format(time),\n",
    "                                                            'total_areas_{}'.format(time),\n",
    "                                                            'areas_count_{}'.format(time),\n",
    "                                                           'max_burst_{}'.format(time),\n",
    "                                                            'max_dq_{}'.format(time),\n",
    "                                                            'err_raw_count_{}'.format(time),\n",
    "                                                            'err_col_count_{}'.format(time),\n",
    "                                                            'err_dq_count_{}'.format(time),\n",
    "                                                            'err_burst_count_{}'.format(time),\n",
    "                                                            'err_raw_count_max_{}'.format(time),\n",
    "                                                            'err_col_count_max_{}'.format(time),])))\n",
    "    tmp1[\"LogTime\"] = tmp2[\"LogTime\"]\n",
    "    return tmp1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5c0d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_addr_features_v2(tmp2,gdf,time):\n",
    "    res= pd.DataFrame() \n",
    "    tmp1 = gdf[['MciAddr','RetryRdErrLogParity']].nunique().reset_index().rename(columns={'MciAddr':'MciAddr_nunique{}'.format(time),\n",
    "                                                                                                     'RetryRdErrLogParity':'Parity_nuique{}'.format(time),                                                                                    })\n",
    "    del tmp1['time_index']\n",
    "\n",
    "    tmp1[\"LogTime\"] = tmp2[\"LogTime\"]\n",
    "\n",
    "    return tmp1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a9799",
   "metadata": {},
   "source": [
    "### Combine Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cab4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_df(*args):\n",
    "    features = args[0]\n",
    "    for df in args[1:]:\n",
    "        try:\n",
    "            features = features.merge(df,on=['sn','LogTime'],how='left')\n",
    "        except:\n",
    "            import traceback\n",
    "            print(traceback.print_exc())\n",
    "            print(df.columns)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96b490b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faults_features_diff1.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4e42e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807540f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81a809cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_sn(sn_file,):\n",
    "    trainset_A_df = pd.read_feather(os.path.join(data_path,sn_file))  \n",
    "    trainset_A_df['sn'] = os.path.basename(sn_file).split('.')[0]\n",
    "    \n",
    "    trainset_A_df = preprocess_data(trainset_A_df)\n",
    "    \n",
    "    train = trainset_A_df[trainset_A_df.LogTime<pd.to_datetime('2024-06-01').timestamp()]\n",
    "    test = trainset_A_df[trainset_A_df.LogTime>pd.to_datetime('2024-06-01').timestamp()]\n",
    " \n",
    "    \n",
    "    train = train[train.LogTime>(train.LogTime.max() - max(select_time_windows))]\n",
    "    test = test[test.LogTime>(test.LogTime.max() - max(select_time_windows))]\n",
    "    \n",
    "    if not os.path.exists(os.path.join(feature_path,'train')):\n",
    "        os.makedirs(os.path.join(feature_path,'train'))\n",
    "    if not os.path.exists(os.path.join(feature_path,'test')):\n",
    "        os.makedirs(os.path.join(feature_path,'test'))\n",
    "        \n",
    "    if not os.path.exists(os.path.join(os.path.join(feature_path,'train'),sn_file.split('.')[0])):\n",
    "        os.makedirs(os.path.join(os.path.join(feature_path,'train'),sn_file.split('.')[0]))\n",
    "    \n",
    "    if not os.path.exists(os.path.join(os.path.join(feature_path,'test'),sn_file.split('.')[0])):\n",
    "        os.makedirs(os.path.join(os.path.join(feature_path,'test'),sn_file.split('.')[0]))\n",
    "    addr_col_ = ['bank_column_key','bank_row_key','bank_key','Bankgroup_key','device_key','rank_key'][::-1]\n",
    "    \n",
    "    def _cal_(sn_file,trainset_A_df,feature_path):\n",
    "        # error_bit_feature = trainset_A_df.RetryRdErrLogParity.apply((lambda x:_get_bit_dq_burst_info(x))).rename(columns=dict(zip([0,1,2,3,4],['bit_count', 'dq_count', 'burst_count', 'max_dq_interval', 'max_burst_interval'])))\n",
    "        # error_bit_feature = trainset_A_df[['sn','LogTime']].merge(error_bit_feature,left_index=True,right_index=True)\n",
    "        \n",
    "        for time_ in select_time_windows[::-1]:\n",
    "            for addr_index in  range(len(['bank_column_key','bank_row_key','bank_key','Bankgroup_key','device_key','rank_key'])):\n",
    "                addr_ = addr_col_[addr_index]\n",
    "                \n",
    "                \n",
    "                trainset_A_df[\"time_index\"] = np.ceil(trainset_A_df[\"LogTime\"]/time_)*time_\n",
    "                gdfcol = ['sn','time_index']+[addr_]\n",
    "                \n",
    "                tmp2 = trainset_A_df[['LogTime']+gdfcol].groupby('time_index').LogTime.max().reset_index()\n",
    "\n",
    "                cols = [addr_,str(time_)]\n",
    "    \n",
    "                gdf = trainset_A_df.groupby(gdfcol)\n",
    "                counts_features =calculate_counts_features_v2(tmp2,gdf,'&'.join(cols))\n",
    "                \n",
    "                \n",
    "                # del counts_features[addr_]\n",
    "    \n",
    "                error_bit_feature2 = calculate_bit_features2_v2(tmp2,gdf,'&'.join(cols))\n",
    "           \n",
    "                del error_bit_feature2[addr_]\n",
    "                \n",
    "\n",
    "\n",
    "            \n",
    "                faults_features =calculate_faults_features_v2(tmp2,gdf,'&'.join(cols),addr_)\n",
    "                \n",
    "                \n",
    "                addr_feature = calculate_addr_features_v2(tmp2,gdf,'&'.join(cols))\n",
    "                \n",
    "                del addr_feature[addr_]\n",
    "                    \n",
    "                ce_storm_feature =calculate_storm_features_v2(tmp2,gdf,'&'.join(cols))\n",
    "                \n",
    "                del ce_storm_feature[addr_]\n",
    "\n",
    "                \n",
    "                    \n",
    "                \n",
    "                trainset_A_df['time_diff'] = trainset_A_df.time_diff.fillna(0) \n",
    "         \n",
    "                gdf = trainset_A_df.groupby(gdfcol)\n",
    "                time_features = calculate_time_features_v2(tmp2,gdf,'&'.join(cols))\n",
    "                \n",
    "                del time_features[addr_]\n",
    "                    \n",
    "                \n",
    "                \n",
    "                counts_features[\"time_index\"] = np.ceil(counts_features[\"LogTime\"]/time_)*time_\n",
    "                faults_features[\"time_index\"] = np.ceil(faults_features[\"LogTime\"]/time_)*time_\n",
    "                \n",
    "                counts_features_gdf = counts_features.groupby(gdfcol)\n",
    "                faults_features_gdf = faults_features.groupby(gdfcol)\n",
    "\n",
    "                del faults_features[addr_]\n",
    "                del counts_features[addr_]\n",
    "        \n",
    "                err_count_statistic_features,faults_statistic_features= get_level2_features_v2(tmp2,counts_features_gdf,faults_features_gdf,'&'.join(cols),addr_)\n",
    "                del err_count_statistic_features[addr_]\n",
    "                del faults_statistic_features[addr_]\n",
    "                    \n",
    "                count_features_diff1,faults_features_diff1 = get_level2_diff_features_v2(counts_features,faults_features)\n",
    "                del counts_features[\"time_index\"]\n",
    "                del faults_features[\"time_index\"]\n",
    "                \n",
    "                def __func():\n",
    "                    features = _merge_df(time_features,\n",
    "                                     faults_features,\n",
    "                                         error_bit_feature2,\n",
    "                                     addr_feature,\n",
    "                                    counts_features,\n",
    "                                    ce_storm_feature,\n",
    "                                         err_count_statistic_features,\n",
    "                                         faults_statistic_features,\n",
    "                                         count_features_diff1,\n",
    "                                         faults_features_diff1)\n",
    "                    fileanme = '&'.join(cols)\n",
    "                \n",
    "                    feather.write_dataframe(\n",
    "                            features,\n",
    "                            os.path.join(feature_path, sn_file.split('.')[0],f'{sn_file.replace(\"csv\", \"feather\")}_{fileanme}'),\n",
    "                        )\n",
    "                import multiprocessing\n",
    "                p = multiprocessing.Process(target=__func,args=())\n",
    "                p.start()\n",
    "                # __func()\n",
    "            \n",
    "               \n",
    "        \n",
    "    if train.shape[0]: _cal_(sn_file,train,os.path.join(feature_path,'train'))\n",
    "    if test.shape[0]:_cal_(sn_file,test,os.path.join(feature_path,'test'))\n",
    "    # print(sn_file,trainset_A_df.shape,time.time()-start,time.time())\n",
    "def worker_initializer():\n",
    "    # 设置子进程为非守护进程\n",
    "    multiprocessing.current_process().daemon = False\n",
    "    \n",
    "def process_all_sn(data_path,feature_path) :\n",
    "        \"\"\"\n",
    "        处理所有 sn 文件, 并保存特征, 支持多进程处理以提高效率\n",
    "        \"\"\"\n",
    "\n",
    "        sn_files = os.listdir(data_path)\n",
    "        sn_files.sort()\n",
    "        with Pool(multiprocessing.cpu_count(), initializer=worker_initializer) as pool:\n",
    "            list(\n",
    "                tqdm(\n",
    "                    pool.imap(process_single_sn,sn_files),\n",
    "                    total=len(sn_files),\n",
    "                    desc=\"Generating features\",\n",
    "                )\n",
    "            )\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950c0d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import time\n",
    "# start = time.time()\n",
    "# data_path=\"type_A/\"\n",
    "# feature_path=\"./feature\"\n",
    "# process_single_sn(\"sn_22027.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feather.read_feather('./feature/type_B/sn_9.feather').columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d955cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "0x0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e7489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f158e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating features:   0%|          | 0/56403 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data_path=\"./SmartHW-main/sample_data/server_type_A/\"\n",
    "data_path=\"type_A/\"\n",
    "feature_path=\"./feature/type_A/\"\n",
    "process_all_sn(data_path,feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glob.glob(f'./feature/*/train/*')[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122033d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_path=\"type_B/\"\n",
    "feature_path=\"./feature/type_B/\"\n",
    "process_all_sn(data_path,feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('./feature/type_A/train/sn_22027/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f225594",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_test = []\n",
    "for sn_file in glob.glob(f'./feature/*/train/*'):\n",
    "    if len(glob.glob(f'{sn_file}/*'))==0:\n",
    "        only_test.append(sn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cffd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(only_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4457fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testlist=[]\n",
    "for sn_file in glob.glob(f'./feature/*/test/*'):\n",
    "    if len(glob.glob(f'{sn_file}/*'))!=0:\n",
    "        testlist.append(sn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist=[]\n",
    "for sn_file in glob.glob(f'./feature/*/train/*'):\n",
    "    if len(glob.glob(f'{sn_file}/*'))!=0:\n",
    "        trainlist.append(sn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12edb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_signal_data(sn_file):\n",
    "    df = pd.DataFrame()\n",
    "    trainset_A = []\n",
    "    if os.path.exists( os.path.join(sn_file,f'{sn_file.split(os.sep)[-1]}_total')):\n",
    "        df = pd.read_feather(os.path.join(sn_file,f'{sn_file.split(os.sep)[-1]}_total'))  \n",
    "        df['sn'] = sn_file.split(os.sep)[-1]\n",
    "        return df\n",
    "    else:\n",
    "        for file in glob.glob(f'{sn_file}/*'):\n",
    "            try:\n",
    "                df = pd.read_feather(file)  # Use pandas.read_feather()\n",
    "                df['sn'] = os.path.basename(file).split('.')[0]  # Extract filename without extension\n",
    "                trainset_A.append(df)\n",
    "                df = _merge_df(*trainset_A)\n",
    "            except:\n",
    "                import traceback\n",
    "                print(file,traceback.print_exc())\n",
    "        if df.shape[0]:\n",
    "                feather.write_dataframe(\n",
    "                    df,\n",
    "                    os.path.join(sn_file,f'{sn_file.split(os.sep)[-1]}_total'),\n",
    "                )\n",
    "    return df\n",
    "\n",
    "def read_all_data(path='./feature/*',mode='train'):\n",
    "    print(f'{path}/{mode}/*')\n",
    "    results = []\n",
    "    resdf = []\n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "        for sn_file in glob.glob(f'{path}/{mode}/*'):\n",
    "            p = pool.apply_async(read_signal_data, (sn_file,))\n",
    "            results.append(p)\n",
    "        for p in results:\n",
    "            tmp = p.get()\n",
    "            if tmp.shape[0]:resdf.append(tmp)\n",
    "    results = pd.concat(resdf)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(glob.glob(f'./feature/*/train/*')[:500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "traindata = read_all_data(path='./feature/*',mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit_count, dq_count, burst_count, max_dq_interval, max_burst_interva/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_count_cols = traindata.columns[traindata.columns.str.contains('bit_count')].tolist()\n",
    "dq_count_cols = traindata.columns[traindata.columns.str.contains('dq_count')].tolist()\n",
    "burst_count_cols = traindata.columns[traindata.columns.str.contains('burst_count')].tolist()\n",
    "max_dq_interval_cols = traindata.columns[traindata.columns.str.contains('max_dq_interval')].tolist()\n",
    "max_burst_interva_cols = traindata.columns[traindata.columns.str.contains('max_burst_interva')].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe532a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7be042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_cols = traindata.columns.difference(bit_count_cols+dq_count_cols+burst_count_cols+max_dq_interval_cols+max_burst_interva_cols+['LogTime', 'anomaly', 'sn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssssss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindata.sn.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ebcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# testdata = read_all_data(path='./feature/*',mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data based on anomalies\n",
    "traindata['anomaly'] = 0\n",
    "traindata.loc[traindata['sn'].isin(ticket['sn_name']), 'anomaly'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2993b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:23:09.278338Z",
     "start_time": "2025-01-10T17:23:09.275430Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# X = traindata[traindata.columns.difference(['sn','LogTime','time_index'])]\n",
    "# y = traindata[['sn','LogTime','anomaly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindata.sn.unique()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.anomaly.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset_A_df.shape,features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split(X, y, test_size=0.1, random_state=42,stratify=y['anomaly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49aef06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:23:10.306227Z",
     "start_time": "2025-01-10T17:23:10.302882Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42,stratify=y['anomaly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36eb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindata[traindata.sn.isin(traindata[traindata.sn.isin(ticket['sn_name'])].sn.drop_duplicates().sample(frac=0.1))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a428434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(traindata,i=2025):\n",
    "  \n",
    "    bad_sn = traindata[traindata.sn.isin(ticket['sn_name'])].sn.drop_duplicates()\n",
    "    good_sn = traindata[~traindata.sn.isin(ticket['sn_name'])].sn.drop_duplicates()\n",
    "    \n",
    "    test_good_sn=good_sn.sample(frac=0.2,random_state=i)\n",
    "    test_bad_sn = bad_sn.sample(frac=0.2,random_state=i)\n",
    "\n",
    "    test_sn = pd.concat([test_good_sn,test_bad_sn])\n",
    "    \n",
    "    test_data = traindata[traindata.sn.isin(test_sn)]\n",
    "    train_data = traindata[~traindata.sn.isin(test_sn)]\n",
    "    \n",
    "    X_train = train_data[train_data.columns.difference(['sn','LogTime','time_index','anomaly'])]\n",
    "    y_train = train_data[['sn','LogTime','anomaly']]\n",
    "    X_test = test_data[test_data.columns.difference(['sn','LogTime','time_index','anomaly'])]\n",
    "    y_test = test_data[['sn','LogTime','anomaly']]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = get_train_test(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(y_test.sn.isin(y_train.sn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed334bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = traindata[traindata.isin(test_sn)]\n",
    "# train_data = traindata[~traindata.isin(test_sn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = train_data[train_data.columns.difference(['sn','LogTime','time_index'])]\n",
    "# y_train = train_data[['sn','LogTime','anomaly']]\n",
    "# X_test = test_data[test_data.columns.difference(['sn','LogTime','time_index'])]\n",
    "# y_test = test_data[['sn','LogTime','anomaly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import derivative\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73dfbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.anomaly.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa897f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 136342/1904280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def focal_loss_lgb(y_pred, dtrain, alpha=0.9, gamma=2):\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    def fl(x,t):\n",
    "        p = 1/(1+np.exp(-x))\n",
    "        return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "    partial_fl = lambda x: fl(x, y_true)\n",
    "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "    return grad, hess\n",
    "\n",
    "def focal_loss_lgb_eval(y_pred, dtrain, alpha=0.97, gamma=2):\n",
    "    a,g = alpha, gamma\n",
    "    y_true = dtrain.label\n",
    "    p = 1/(1+np.exp(-y_pred))\n",
    "    loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n",
    "    return 'focal_loss', np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59a4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed666f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate':0.1,\n",
    "              'num_leaves':127,\n",
    "              'subsample':1,\n",
    "              'colsample_bytree':1,\n",
    "              'random_state':4,            \n",
    "              'metric':'focal_loss_lgb_eval_error',\n",
    "          'objective':focal_loss_lgb,\n",
    "              'num_threads':-1,       \n",
    "          'verbose':1,\n",
    "          'early_stop':True,\n",
    "        }\n",
    "lgb_train=lightgbm.Dataset(X_train[select_cols], label=y_train['anomaly'])\n",
    "# lgb_test = xgb.DMatrix(X_test, label=y_test['anomaly'], enable_categorical=True)\n",
    "lgbm = lightgbm.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets = lgb_train,\n",
    "    feval=focal_loss_lgb_eval_error,\n",
    "    num_boost_round = 50,\n",
    "    # early_stopping_rounds=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_test = lightgbm.Dataset(X_test, label=y_test['anomaly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508531f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtrain_xgb.get_label()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfe831",
   "metadata": {},
   "source": [
    "### Model Training with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de027a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:23:11.958954Z",
     "start_time": "2025-01-10T17:23:11.821519Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%time\n",
    "# dtrain_xgb = xgb.DMatrix(X_train, label=y_train['anomaly'], enable_categorical=True)\n",
    "# dtest_xgb = xgb.DMatrix(X_test, label=y_test['anomaly'], enable_categorical=True)\n",
    "\n",
    "# # XGBoost model parameters\n",
    "# params_xgb = {\n",
    "#     'objective': 'binary:logistic',\n",
    "#     'tree_method': 'hist',\n",
    "#     'eval_metric': 'logloss',\n",
    "#     'random_state': 42,\n",
    "#     # 'verbose':0,\n",
    "#     # 'num_leaves':127,\n",
    "#     # 'subsample':1,\n",
    "#     # 'colsample_bytree':1,\n",
    "#     # 'random_state':4, \n",
    "#     # 'learning_rate':0.06,\n",
    "# }\n",
    "\n",
    "# # Custom callback for progress bar\n",
    "# class XGBoostProgressCallback(xgb.callback.TrainingCallback):\n",
    "#     def __init__(self, total):\n",
    "#         self.pbar = tqdm(total=total, desc=\"XGBoost Training Progress\")\n",
    "\n",
    "#     def after_iteration(self, model, epoch, evals_log):\n",
    "#         self.pbar.update(1)\n",
    "#         if epoch + 1 == self.pbar.total:\n",
    "#             self.pbar.close()\n",
    "#         return False\n",
    "# def custom_metric(predt, dtrain):\n",
    "#     label = dtrain.get_label()\n",
    "#     error = np.mean((predt - label) ** 2) \n",
    "#     return \"custom_error\", error\n",
    "\n",
    "# early_stop = xgb.callback.EarlyStopping(rounds=10, \n",
    "#                                         metric_name=\"custom_error\", \n",
    "#                                         data_name=\"test\")\n",
    "\n",
    "# # Train the model with progress bar\n",
    "# num_boost_round = 100\n",
    "# evals_result_xgb = {}\n",
    "# bst_xgb = xgb.train(params_xgb, dtrain_xgb, \n",
    "#                     num_boost_round=num_boost_round,\n",
    "#                     evals=[(dtest_xgb, 'test')],\n",
    "#                     evals_result=evals_result_xgb,\n",
    "#                     early_stopping_rounds=True,\n",
    "#                     # obj=focal_loss_lgb\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "# y_pred_xgb = bst_xgb.predict(dtest_xgb)\n",
    "y_pred_xgb =lgbm.predict(X_test[select_cols])\n",
    "# y_pred_xgb_binary = [1 if y > 0.5 else 0 for y in y_pred_xgb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18f85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_score(y_test,ticket,lgbm,select_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5803ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_score(y_test,ticket,lgbm,select_cols):\n",
    "    y_pred_xgb =lgbm.predict(X_test[select_cols])\n",
    "    subtest = y_test\n",
    "    subtest = subtest.merge(ticket,left_on=['sn'],right_on=['sn_name'],how='left')\n",
    "    subtest['y_pred_xgb'] = y_pred_xgb\n",
    "    res = []\n",
    "    for i in range(1,100,1):\n",
    "        sub = subtest\n",
    "        sub['pred'] = sub['y_pred_xgb'] > (i/100)\n",
    "        sub['pred_t'] = (sub[sub.pred==1].alarm_time - sub[sub.pred==1].LogTime>15*60) &(sub[sub.pred==1].alarm_time - sub[sub.pred==1].LogTime<7*24*60*60+15*60)\n",
    "        try:\n",
    "            precise = sub[sub.pred_t==True].sn.nunique()/sub[sub.pred==1].sn.nunique()\n",
    "        except:\n",
    "            precise=0\n",
    "        try:\n",
    "            recall = sub[sub.pred_t==True].sn.nunique()/sub[sub.anomaly==1].sn.nunique()\n",
    "        except:\n",
    "            recall = 0\n",
    "        try:\n",
    "            f1 = 2*precise*recall/(precise+recall)\n",
    "        except:\n",
    "            f1=0\n",
    "        res.append({\"precise\":precise,\"recall\":recall,\"f1\":f1,\"threshold\":i/100,'ntpp':tp,'npp':sub[sub.pred==1].sn.nunique(),\"ntpr\":sub[sub.anomaly==1].sn.nunique()})\n",
    "    tmp = pd.DataFrame(res).sort_values('f1').tail(1)\n",
    "    # .tail(1)\n",
    "    # try:\n",
    "    #     f.write('平均分为:{},probability:{}\\n'.format(tmp.f1.values(0),tmp.threshold.values(0)))\n",
    "    #     f.flush()\n",
    "    # except Exception:\n",
    "    #     print('平均分为:{},probability:{}\\n'.format(tmp.f1.values(0),tmp.threshold.values(0)))\n",
    "\n",
    "    \n",
    "    try:\n",
    "        f.write(str(tmp.to_dict(\"index\"))+'\\n')\n",
    "        f.flush()\n",
    "    except Exception:\n",
    "        print(tmp.to_dict(\"index\"))\n",
    "    return tmp.f1.values[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8176b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).sort_values('f1').tail(1).f1.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(res).sort_values('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(res).sort_values('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80138a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffffffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a,g = 0.9, 2\n",
    "seed = 7\n",
    "test_size = 0.1\n",
    "\n",
    "def focal_loss_lgb(y_true, y_pred):\n",
    "    def fl(x,t):\n",
    "        p = 1/(1+np.exp(-x))\n",
    "        return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "    partial_fl = lambda x: fl(x, y_true)\n",
    "    grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "    hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "    return grad, hess\n",
    "  \n",
    "def focal_loss_lgb_eval(y_true, y_pred):\n",
    "    p = 1/(1+np.exp(-y_pred))\n",
    "    loss = -( a*y_true + (1-a)*(1-y_true) ) * (( 1 - ( y_true*p + (1-y_true)*(1-p)) )**g) * ( y_true*np.log(p)+(1-y_true)*np.log(1-p) )\n",
    "    return 'focal_loss', np.mean(loss), False\n",
    "# split data into train and test sets\n",
    "def ga_train_clf(features_name, X_train, X_test, y_train, y_test):\n",
    "    clf = LGBMClassifier(\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=50,\n",
    "        num_leaves=127,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=2025,\n",
    "        metric=['auc'],\n",
    "        n_jobs = -1,\n",
    "        objective=focal_loss_lgb,\n",
    "        # categorical_column ='model'\n",
    "        # earl\n",
    "\n",
    "    )\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(train_x[features_name], train_y, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "\n",
    "    clf.fit(\n",
    "        X_train[features_name], y_train['anomaly'],\n",
    "        # eval_set=[(X_train[features_name], y_train),(X_test[features_name], y_test)],\n",
    "        # verbose=0,  \n",
    "        eval_metric='auc',\n",
    "    )\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    " ga_train_clf(select_cols, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./test_lgb_v2.txt','w')\n",
    "used_columns = X_train.columns.difference(['sn','LogTime','time_index','anomaly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORE_NONE = -1\n",
    "\n",
    "class Life(object):\n",
    "      \"\"\"个体类\"\"\"\n",
    "      def __init__(self, aGene=None):\n",
    "            self.gene = aGene\n",
    "            self.score = SCORE_NONE  # 初始化生命值  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "#from Life import Life\n",
    "import numpy as np\n",
    "\n",
    "class GA(object):\n",
    "    \"\"\"遗传算法类\"\"\"\n",
    "\n",
    "    def __init__(self, aCrossRate, aMutationRage, aLifeCount, aGeneLenght, aMatchFun=lambda life: 1):\n",
    "        self.croessRate = aCrossRate  # 交叉概率 #\n",
    "        self.mutationRate = aMutationRage  # 突变概率 #\n",
    "        self.lifeCount = aLifeCount   # 个体数 #\n",
    "        self.geneLenght = aGeneLenght  # 基因长度 #\n",
    "        self.matchFun = aMatchFun  # 适配函数\n",
    "        self.lives = []  # 种群\n",
    "        # self.best = None  # 保存这一代中最好的个体\n",
    "        self.best = Life(np.random.randint(0, 2, self.geneLenght))  # 保存这一代中最好的个体\n",
    "\n",
    "        self.gene = np.random.randint(0, 2, self.geneLenght)  # 保存全局最好的个体 #\n",
    "        self.score = -1   # 保存全局最高的适应度 #\n",
    "\n",
    "        self.generation = 0  # 第几代 #\n",
    "        self.crossCount = 0  # 交叉数量 #\n",
    "        self.mutationCount = 0  # 突变个数 #\n",
    "        self.bounds = 0.0  # 适配值之和，用于选择时计算概率\n",
    "        self.initPopulation()  # 初始化种群 #\n",
    "\n",
    "    def initPopulation(self):\n",
    "        \"\"\"初始化种群\"\"\"\n",
    "        self.lives = []\n",
    "        count = 0\n",
    "        while count < self.lifeCount:\n",
    "            gene = np.random.randint(0, 2, self.geneLenght)\n",
    "            life = Life(gene)\n",
    "            random.shuffle(gene)  # 随机洗牌 #\n",
    "            self.lives.append(life)\n",
    "            count += 1\n",
    "\n",
    "    def judge(self):\n",
    "        \"\"\"评估，计算每一个个体的适配值\"\"\"\n",
    "        self.bounds = 0.0\n",
    "        # self.best = self.lives[0]\n",
    "        self.best.score = copy.deepcopy(self.score)  ####\n",
    "        self.best.gene = copy.deepcopy(self.gene)  ####\n",
    "        for life in self.lives:\n",
    "            life.score = self.matchFun(life)\n",
    "            self.bounds += life.score\n",
    "            if self.best.score < life.score:     # score为auc 越大越好 #\n",
    "                self.best = life\n",
    "\n",
    "        if self.score < self.best.score:                          ####\n",
    "            self.score = copy.deepcopy(self.best.score)           ####\n",
    "            self.gene = copy.deepcopy(self.best.gene)             ####\n",
    "\n",
    "        # self.best.score = copy.deepcopy(self.score)               ####\n",
    "        # self.best.gene = copy.deepcopy(self.gene)                 ####\n",
    "\n",
    "    def cross(self, parent1, parent2):\n",
    "        \"\"\"\n",
    "        函数功能：交叉\n",
    "        \"\"\"\n",
    "        index1 = random.randint(0, self.geneLenght - 1)  # 随机生成突变起始位置 #\n",
    "        index2 = random.randint(index1, self.geneLenght - 1)  # 随机生成突变终止位置 #\n",
    "\n",
    "        for index in range(len(parent1.gene)):\n",
    "            if (index >= index1) and (index <= index2):\n",
    "                parent1.gene[index], parent2.gene[index] = parent2.gene[index], parent1.gene[index]\n",
    "\n",
    "        self.crossCount += 1\n",
    "        return parent1.gene\n",
    "\n",
    "    def mutation(self, gene):\n",
    "        \"\"\"突变\"\"\"\n",
    "        index1 = random.randint(0, self.geneLenght - 1)\n",
    "        index2 = random.randint(0, self.geneLenght - 1)\n",
    "        # 随机选择两个位置的基因交换--变异 #\n",
    "        newGene = gene[:]  # 产生一个新的基因序列，以免变异的时候影响父种群\n",
    "        newGene[index1], newGene[index2] = newGene[index2], newGene[index1]\n",
    "        self.mutationCount += 1\n",
    "        return newGene\n",
    "\n",
    "    def getOne(self):\n",
    "        \"\"\"选择一个个体\"\"\"\n",
    "        r = random.uniform(0, self.bounds)\n",
    "        for life in self.lives:\n",
    "            r -= life.score\n",
    "            if r <= 0:\n",
    "                return life\n",
    "\n",
    "        raise Exception(\"选择错误\", self.bounds)\n",
    "\n",
    "    def newChild(self):\n",
    "        \"\"\"产生新的后代\"\"\"\n",
    "        parent1 = self.getOne()\n",
    "        rate = random.random()\n",
    "\n",
    "        # 按概率交叉 #\n",
    "        if rate < self.croessRate:\n",
    "            # 交叉 #\n",
    "            parent2 = self.getOne()\n",
    "            gene = self.cross(parent1, parent2)\n",
    "        else:\n",
    "            gene = parent1.gene\n",
    "\n",
    "        # 按概率突变 #\n",
    "        rate = random.random()\n",
    "        if rate < self.mutationRate:\n",
    "            gene = self.mutation(gene)\n",
    "\n",
    "        return Life(gene)\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"产生下一代\"\"\"\n",
    "        self.judge()\n",
    "        newLives = []\n",
    "        newLives.append(self.best)  # 把最好的个体加入下一代 #\n",
    "        newLives[0].gene = copy.deepcopy(self.gene)\n",
    "        newLives[0].score = copy.deepcopy(self.score)\n",
    "        while len(newLives) < self.lifeCount:\n",
    "            newLives.append(self.newChild())\n",
    "        self.lives = newLives\n",
    "        self.generation += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843fe958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "#from Genetic_algorithm import GA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "class FeatureSelection(object):\n",
    "    def __init__(self, aLifeCount=10):\n",
    "        self.columns = used_columns\n",
    "        self.lifeCount = aLifeCount\n",
    "        self.ga = GA(aCrossRate=0.7,\n",
    "                     aMutationRage=0.8,\n",
    "                     aLifeCount=self.lifeCount,\n",
    "                     aGeneLenght=len(self.columns),\n",
    "                     aMatchFun=self.matchFun())\n",
    "\n",
    "    def auc_score(self, order):\n",
    "        #print(order)\n",
    "        features = self.columns\n",
    "        features_name = []\n",
    "        for index in range(len(order)):\n",
    "            if order[index] == 1:\n",
    "                features_name.append(features[index])\n",
    "#         print(features_name)\n",
    "        f.write(str(features_name)+'\\n')\n",
    "        f.flush()\n",
    "    \n",
    "        # clf = ga_train_clf(features_name, train_x, train_y)\n",
    "        clf = ga_train_clf(features_name, X_train, X_test, y_train, y_test)\n",
    "        best_score = get_best_score(y_test,ticket,clf,features_name)\n",
    "        \n",
    "        f.write('平均分:{}\\n'.format(str(best_score)))      \n",
    "        f.flush()\n",
    "        return best_score\n",
    "\n",
    "    def matchFun(self):\n",
    "        return lambda life: self.auc_score(life.gene)\n",
    "\n",
    "    def run(self, n=0):\n",
    "        distance_list = []\n",
    "        generate = [index for index in range(1, n + 1)]\n",
    "        while n > 0:\n",
    "            self.ga.next()\n",
    "            distance = self.auc_score(self.ga.best.gene)\n",
    "            #distance = self.ga.score                      ####\n",
    "            distance_list.append(distance)\n",
    "            print((\"第%d代 : 当前最好特征组合的线下验证结果为：%f\") % (self.ga.generation, distance))\n",
    "            f.write((\"第%d代 : 当前最好特征组合的线下验证结果为：%f\\n\" ) % (self.ga.generation, distance))\n",
    "            f.flush()\n",
    "            n -= 1\n",
    "# \n",
    "        print('当前最好特征组合:')\n",
    "        f.write('当前最好特征组合:\\n')\n",
    "        f.flush()\n",
    "        string = []\n",
    "        flag = 0\n",
    "        features = self.columns[1:]\n",
    "        for index in self.ga.gene:                                  ####\n",
    "            if index == 1:\n",
    "                string.append(features[flag])\n",
    "            flag += 1\n",
    "#         print(string)\n",
    "        f.write(str(string)+'\\n')\n",
    "        f.write('线下最高为auc：{}\\n'.format(self.ga.score))\n",
    "        print('线下最高为auc：{}'.format(self.ga.score))   \n",
    "        f.flush()\n",
    "        ####\n",
    "\n",
    "        '''画图函数'''\n",
    "        plt.plot(generate, distance_list)\n",
    "        plt.xlabel('generation')\n",
    "        plt.ylabel('distance')\n",
    "        plt.title('generation--auc-score')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    fs = FeatureSelection(aLifeCount=20)\n",
    "    rounds = 100    # 算法迭代次数 #\n",
    "    fs.run(rounds)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd56ea62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17501528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df817aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import defaultdict\n",
    "sn_type_list = []\n",
    "for i in glob.glob('/data1/smartmem/stage1_feather/*/*.feather'):\n",
    "    type_ = i.split(os.sep)[-2].split('_')[1]\n",
    "    sn_ = i.split(os.sep)[-1].split('.')[0]\n",
    "    sn_type_list.append({'sn':sn_,'serial_number_type':type_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e9670",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('submission.csv').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aedb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_type = pd.DataFrame(sn_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be88d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dtest = xgb.DMatrix(testdata[X_test.columns], enable_categorical=True)\n",
    "# test_data_y =  bst_xgb.predict(dtest)\n",
    "# test_data_y = [1 if y > 0.3 else 0 for y in test_data_y]\n",
    "# submit = testdata[['sn','LogTime']].merge(sn_type,on='sn')\n",
    "# testdata.columns = ['sn_name', 'prediction_timestamp', 'serial_number_type']\n",
    "# testdata.to_csv('./submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416870e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae09b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46851f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1bfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47321970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55317e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 53/167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41bf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticket['time'] =ticket.alarm_time - 15*60 -7*24*3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = sub.merge(ticket,left_on=['sn'],right_on=['sn_name'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ad28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp = sum((sub[sub.pred==1].alarm_time - sub[sub.pred==1].LogTime>15*60) &(sub[sub.pred==1].alarm_time - sub[sub.pred==1].LogTime<7*24*60*60+15*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea198e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub[sub.pred==1].shape[0],sub[sub.anomaly==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precise = tp/sub[sub.pred==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11456800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall = tp/sub[sub.anomaly==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e483b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = 2*precise*recall/(precise+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c8c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:23:15.217278Z",
     "start_time": "2025-01-10T17:23:15.207843Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa81dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad88e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T17:23:17.378875Z",
     "start_time": "2025-01-10T17:23:17.324378Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importances for XGBoost\n",
    "xgb.plot_importance(bst_xgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17918783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d092b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(lgbm.feature_name(),lgbm.feature_importance()):\n",
    "    if j>0:\n",
    "        print(i,j)\n",
    "        cols.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad322549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
